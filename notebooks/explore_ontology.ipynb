{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ontological connection checking\n",
    "\n",
    "Here I want to play around with ontological type checking in `pyiron_workflow` using `semantikon`'s `u` annotations."
   ],
   "id": "f672db8683348bf7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:33.766682Z",
     "start_time": "2025-07-23T19:41:32.132888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rdflib import Namespace, RDFS\n",
    "\n",
    "import pyiron_workflow as pwf\n",
    "from semantikon.metadata import u\n",
    "from semantikon import ontology  # get_knowledge_graph, validate_values\n",
    "from pyiron_ontology import parser  # export_to_dict, parse_workflow\n",
    "from pyiron_workflow.channels import ChannelConnectionError\n",
    "from pyiron_workflow.nodes.composite import FailedChildError\n",
    "\n",
    "\n",
    "EX = Namespace(\"http://www.example.org/\")\n",
    "\n",
    "class Meal: ...\n",
    "\n",
    "class Garbage: ...\n",
    "\n",
    "@pwf.as_function_node(\"pizza\")\n",
    "def prepare_pizza() -> u(Meal, uri=EX.Pizza):\n",
    "    return Meal()\n",
    "\n",
    "@pwf.as_function_node(\"unidentified_meal\")\n",
    "def prepare_non_ontological_meal() -> Meal:\n",
    "    return Meal()\n",
    "\n",
    "@pwf.as_function_node(\"rice\")\n",
    "def prepare_rice() -> u(Meal, uri=EX.Rice):\n",
    "    return Meal()\n",
    "\n",
    "@pwf.as_function_node(\"garbage\")\n",
    "def prepare_garbage() -> u(Garbage, uri=EX.Garbage):\n",
    "    return Garbage()\n",
    "\n",
    "@pwf.as_function_node(\"garbage\")\n",
    "def prepare_unhinted_garbage():\n",
    "    return Garbage()\n",
    "\n",
    "@pwf.as_function_node(\"verdict\")\n",
    "def eat(meal: u(Meal, uri=EX.Meal)) -> str:\n",
    "    return f\"Yummy {meal.__class__.__name__} meal\"\n",
    "\n",
    "@pwf.as_function_node(\"verdict\")\n",
    "def eat_pizza(meal: u(Meal, uri=EX.Pizza)) -> str:\n",
    "    return f\"Yummy {meal.__class__.__name__} pizza\""
   ],
   "id": "996ce3d6152f1beb",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:33.849449Z",
     "start_time": "2025-07-23T19:41:33.843931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Both fully hinted\n",
    "wf = pwf.Workflow(\"ontoflow\")\n",
    "wf.make = prepare_pizza()\n",
    "wf.eat = eat_pizza(wf.make)\n",
    "wf()"
   ],
   "id": "d9f4c5df1ee62b88",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eat__verdict': 'Yummy Meal pizza'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:33.983981Z",
     "start_time": "2025-07-23T19:41:33.856302Z"
    }
   },
   "cell_type": "code",
   "source": "ontology.validate_values(parser.parse_workflow(wf))",
   "id": "65ab92b446af209",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_triples': [], 'incompatible_connections': []}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:33.998243Z",
     "start_time": "2025-07-23T19:41:33.992243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Upstream type hint is missing\n",
    "wf = pwf.Workflow(\"no_type\")\n",
    "wf.make = prepare_unhinted_garbage()\n",
    "wf.eat = eat_pizza(wf.make)\n",
    "try:\n",
    "    wf()\n",
    "except FailedChildError as e:\n",
    "    print(e)"
   ],
   "id": "5279fb9d04b9cb99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/no_type encountered error in child: {'/no_type/eat.accumulate_and_run': TypeError(\"The channel /no_type/eat.meal cannot take the value `<__main__.Garbage object at 0x13e1bb9e0>` (<class '__main__.Garbage'>) because it is not compliant with the type hint typing.Annotated[__main__.Meal, ('uri', rdflib.term.URIRef('http://www.example.org/Pizza'))]\")}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.008576Z",
     "start_time": "2025-07-23T19:41:34.005032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Upstream type hint is wrong\n",
    "wf = pwf.Workflow(\"no_type\")\n",
    "wf.make = prepare_garbage()\n",
    "try:\n",
    "    wf.eat = eat_pizza(wf.make)\n",
    "except ChannelConnectionError as e:\n",
    "    print(e)"
   ],
   "id": "a530586921463886",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The channel /no_type/make.garbage (<class 'pyiron_workflow.mixin.injection.OutputDataWithInjection'>) has the correct type (<class 'pyiron_workflow.channels.OutputData'>) to connect with /eat_pizza.meal (<class 'pyiron_workflow.channels.InputData'>), but is not a valid connection.Please check type hints, etc. /no_type/make.garbage.type_hint = typing.Annotated[__main__.Garbage, ('uri', rdflib.term.URIRef('http://www.example.org/Garbage'))]; /eat_pizza.meal.type_hint = typing.Annotated[__main__.Meal, ('uri', rdflib.term.URIRef('http://www.example.org/Pizza'))]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So far, so good: `u` decoration has no negative impact on the existing type hint checking procedures",
   "id": "5995ff2b2e009aee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.021383Z",
     "start_time": "2025-07-23T19:41:34.017185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Upstream ontological hint is missing\n",
    "wf = pwf.Workflow(\"no_ontology\")\n",
    "wf.make = prepare_non_ontological_meal()\n",
    "wf.eat = eat_pizza(wf.make)\n",
    "wf()\n",
    "# This should stop working once I implement and activate ontological typing: ontological hint missing\n",
    "# I guess that unlike a missing type hint, where we may get lucky at runtime and receive the right type,\n",
    "# here we should fail at connection time"
   ],
   "id": "d6d324aff20c4e63",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eat__verdict': 'Yummy Meal pizza'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.071787Z",
     "start_time": "2025-07-23T19:41:34.032603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ontology.validate_values(parser.parse_workflow(wf))\n",
    "# Or should it stop? `validate_values` doesn't seem to care..."
   ],
   "id": "66353db1da92b620",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_triples': [], 'incompatible_connections': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.081930Z",
     "start_time": "2025-07-23T19:41:34.077826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Upstream ontological hint is WRONG\n",
    "wf = pwf.Workflow(\"failed_ontology\")\n",
    "wf.make = prepare_rice()\n",
    "# This should stop working once I implement and activate ontological typing: ontological hint mismatch\n",
    "wf.eat = eat_pizza(wf.make)\n",
    "wf()"
   ],
   "id": "316fea01813d2c64",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eat__verdict': 'Yummy Meal pizza'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.131701Z",
     "start_time": "2025-07-23T19:41:34.093760Z"
    }
   },
   "cell_type": "code",
   "source": "ontology.validate_values(parser.parse_workflow(wf))",
   "id": "a187aa287206a413",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_triples': [],\n",
       " 'incompatible_connections': [(rdflib.term.URIRef('failed_ontology.eat.inputs.meal'),\n",
       "   rdflib.term.URIRef('failed_ontology.make.outputs.rice'),\n",
       "   [rdflib.term.URIRef('http://www.example.org/Pizza')],\n",
       "   [rdflib.term.URIRef('http://www.example.org/Rice')])]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.144291Z",
     "start_time": "2025-07-23T19:41:34.140279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Downstream ontological hint is less specific\n",
    "wf = pwf.Workflow(\"relaxed_ontology\")\n",
    "wf.make = prepare_rice()\n",
    "wf.eat = eat(wf.make)\n",
    "wf()\n",
    "# This should work fine"
   ],
   "id": "230dba9264a053e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eat__verdict': 'Yummy Meal meal'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.191404Z",
     "start_time": "2025-07-23T19:41:34.153244Z"
    }
   },
   "cell_type": "code",
   "source": "ontology.validate_values(parser.parse_workflow(wf))",
   "id": "63f9a48a847ae03e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_triples': [],\n",
       " 'incompatible_connections': [(rdflib.term.URIRef('relaxed_ontology.eat.inputs.meal'),\n",
       "   rdflib.term.URIRef('relaxed_ontology.make.outputs.rice'),\n",
       "   [rdflib.term.URIRef('http://www.example.org/Meal')],\n",
       "   [rdflib.term.URIRef('http://www.example.org/Rice')])]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is not desirable, but is a known outcome from the semantikon notebook. The issue is that we need to inform the graph of the subclass relationship",
   "id": "9a332e6a04dc2bd2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.235865Z",
     "start_time": "2025-07-23T19:41:34.196931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graph = parser.parse_workflow(wf)\n",
    "graph.add((EX.Rice, RDFS.subClassOf, EX.Meal))\n",
    "ontology.validate_values(graph)"
   ],
   "id": "52630cb18cc27959",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_triples': [], 'incompatible_connections': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the pizza example we need to first get the knowledge graph _then_ tell it that `EX.Pizza` is a subclass of `EX.Meal`. I believe I will certainly need to be able to fulfill `get_workflow_dict` from inside `pyiron_workflow`, but I need to see how it will be possible to get these sort of subclass registrations working universally and not for some specific knowledge graph... To this end, the clothing example might actually be _easier_, because there is no extra-graph step...",
   "id": "db502e774603d657"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ontological triples\n",
    "\n",
    "Ok, that's not perfect but it's pretty damned good; let's try it with the clothes example"
   ],
   "id": "574ac2f3813504ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.248235Z",
     "start_time": "2025-07-23T19:41:34.242349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rdflib import OWL, Namespace\n",
    "\n",
    "import pyiron_workflow as pwf\n",
    "from semantikon.metadata import u\n",
    "from semantikon import ontology\n",
    "from pyiron_ontology import parser\n",
    "\n",
    "EX = Namespace(\"http://www.example.org/\")\n",
    "\n",
    "class Clothes:\n",
    "    pass\n",
    "\n",
    "@pwf.as_function_node\n",
    "def wash(clothes: Clothes) -> u(\n",
    "    Clothes,\n",
    "    triples=(EX.hasProperty, EX.cleaned),\n",
    "    derived_from=\"inputs.clothes\"\n",
    "):\n",
    "    ...\n",
    "    return clothes\n",
    "\n",
    "@pwf.as_function_node\n",
    "def dye(clothes: Clothes, color=\"blue\") -> u(\n",
    "    Clothes,\n",
    "    triples=(EX.hasProperty, EX.color),\n",
    "    derived_from=\"inputs.clothes\",\n",
    "):\n",
    "    ...\n",
    "    return clothes\n",
    "\n",
    "@pwf.as_function_node\n",
    "def sell(\n",
    "    clothes: u(\n",
    "        Clothes, restrictions=(\n",
    "            ((OWL.onProperty, EX.hasProperty), (OWL.someValuesFrom, EX.cleaned)),\n",
    "            ((OWL.onProperty, EX.hasProperty), (OWL.someValuesFrom, EX.color))\n",
    "        )\n",
    "    )\n",
    ") -> int:\n",
    "    return 10"
   ],
   "id": "dceaab6f57226f07",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.265784Z",
     "start_time": "2025-07-23T19:41:34.260184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_correct_wf = pwf.Workflow(\"my_correct_workflow\")\n",
    "my_correct_wf.dyed_clothes = dye(Clothes())\n",
    "my_correct_wf.washed_clothes = wash(my_correct_wf.dyed_clothes)\n",
    "my_correct_wf.money = sell(my_correct_wf.washed_clothes)\n",
    "my_correct_wf()"
   ],
   "id": "b009aa84a176b6f5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'money__10': 10}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.359569Z",
     "start_time": "2025-07-23T19:41:34.273531Z"
    }
   },
   "cell_type": "code",
   "source": "ontology.validate_values(parser.parse_workflow(my_correct_wf))",
   "id": "5499d5efa1862b30",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_triples': [], 'incompatible_connections': []}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.380494Z",
     "start_time": "2025-07-23T19:41:34.373029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@pwf.as_macro_node\n",
    "def my_correct_macro(self, clothes: Clothes):\n",
    "    self.dyed_clothes = dye(clothes)\n",
    "    self.washed_clothes = wash(self.dyed_clothes)\n",
    "    self.money = sell(self.washed_clothes)\n",
    "    return self.money\n",
    "\n",
    "correct_m = my_correct_macro(Clothes())\n",
    "correct_m()"
   ],
   "id": "22c536ea480c7c36",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'money': 10}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.488358Z",
     "start_time": "2025-07-23T19:41:34.395232Z"
    }
   },
   "cell_type": "code",
   "source": "ontology.validate_values(parser.parse_workflow(correct_m))",
   "id": "e319f2ea6db43477",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_triples': [], 'incompatible_connections': []}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.507196Z",
     "start_time": "2025-07-23T19:41:34.502167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_wrong_wf = pwf.Workflow(\"my_wrong_workflow\")\n",
    "my_wrong_wf.washed_clothes = wash(Clothes())\n",
    "my_wrong_wf.money = sell(my_wrong_wf.washed_clothes)\n",
    "my_wrong_wf()"
   ],
   "id": "ef51d73b476d6a0b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'money__10': 10}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.594614Z",
     "start_time": "2025-07-23T19:41:34.520679Z"
    }
   },
   "cell_type": "code",
   "source": "ontology.validate_values(parser.parse_workflow(my_wrong_wf))",
   "id": "d8b45bde3eda7aa9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_triples': [(rdflib.term.URIRef('my_wrong_workflow.money.inputs.clothes'),\n",
       "   rdflib.term.URIRef('http://www.example.org/hasProperty'),\n",
       "   rdflib.term.URIRef('http://www.example.org/color'))],\n",
       " 'incompatible_connections': []}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.610573Z",
     "start_time": "2025-07-23T19:41:34.604724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@pwf.as_macro_node\n",
    "def my_wrong_macro(self, clothes: Clothes):\n",
    "    self.washed_clothes = wash(clothes)\n",
    "    self.money = sell(self.washed_clothes)\n",
    "    return self.money\n",
    "\n",
    "wrong_m = my_wrong_macro(Clothes())\n",
    "wrong_m()"
   ],
   "id": "1520f248cef9296e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'money': 10}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.688874Z",
     "start_time": "2025-07-23T19:41:34.622708Z"
    }
   },
   "cell_type": "code",
   "source": "ontology.validate_values(parser.parse_workflow(wrong_m))",
   "id": "b59cfa8b01752473",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_triples': [(rdflib.term.URIRef('my_wrong_macro.money.inputs.clothes'),\n",
       "   rdflib.term.URIRef('http://www.example.org/hasProperty'),\n",
       "   rdflib.term.URIRef('http://www.example.org/color'))],\n",
       " 'incompatible_connections': []}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.699511Z",
     "start_time": "2025-07-23T19:41:34.696491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@pwf.as_function_node\n",
    "def dye_with_cancel(clothes: Clothes, color=\"blue\") -> u(\n",
    "    Clothes,\n",
    "    triples=(EX.hasProperty, EX.color),\n",
    "    derived_from=\"inputs.clothes\",\n",
    "    cancel=(EX.hasProperty, EX.cleaned)\n",
    "):\n",
    "    return clothes"
   ],
   "id": "3055fbc547280d91",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.716962Z",
     "start_time": "2025-07-23T19:41:34.711447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_wf_with_wrong_order = pwf.Workflow(\"my_workflow_with_wrong_order\")\n",
    "my_wf_with_wrong_order.washed_clothes = wash(Clothes())\n",
    "my_wf_with_wrong_order.dyed_clothes = dye_with_cancel(my_wf_with_wrong_order.washed_clothes)\n",
    "my_wf_with_wrong_order.money = sell(my_wf_with_wrong_order.dyed_clothes)\n",
    "my_wf_with_wrong_order()"
   ],
   "id": "912ab5035ea69c03",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'money__10': 10}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.829901Z",
     "start_time": "2025-07-23T19:41:34.730215Z"
    }
   },
   "cell_type": "code",
   "source": "ontology.validate_values(parser.parse_workflow(my_wf_with_wrong_order))",
   "id": "8e590c0187917feb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_triples': [(rdflib.term.URIRef('my_workflow_with_wrong_order.money.inputs.clothes'),\n",
       "   rdflib.term.URIRef('http://www.example.org/hasProperty'),\n",
       "   rdflib.term.URIRef('http://www.example.org/cleaned'))],\n",
       " 'incompatible_connections': []}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This example produces expected outcomes the whole way through.\n",
    "\n",
    "As a first stage then, we can internally create type-validated connections, search for a graph root, transform the entire workflow graph to a \"parsed\" graph, feed that to semantikon, and reverse the connection iff we encounter a problem.\n",
    "This is not the most computationally efficient approach, but should be pretty robust and very fast to implement."
   ],
   "id": "ed2b6333d5c3be36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Going further?\n",
    "\n",
    "Ok, what about if we make it a macro?\n",
    "How does validation proceed making a connection _inside_ a subgraph?\n",
    "If the workflow->graph is parsing subgraphs fine, and the graph->ontological validation is fine, then this might get inefficient but should work out-of-the-box."
   ],
   "id": "997a822981555fef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# End of section",
   "id": "1b037c0824a638e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:41:34.840878Z",
     "start_time": "2025-07-23T19:41:34.839688Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4f683214aef5fd2b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
