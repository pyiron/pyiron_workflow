{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ontological connection checking\n",
    "\n",
    "Here I want to play around with ontological type checking in `pyiron_workflow` using `semantikon`'s `u` annotations."
   ],
   "id": "f672db8683348bf7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.381313Z",
     "start_time": "2025-08-15T22:28:33.373181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import rdflib\n",
    "\n",
    "from semantikon.metadata import u\n",
    "from semantikon import ontology\n",
    "\n",
    "import pyiron_workflow as pwf\n",
    "from pyiron_workflow import knowledge, suggest\n",
    "from pyiron_workflow.channels import ChannelConnectionError\n",
    "from pyiron_workflow.nodes.composite import FailedChildError\n",
    "\n",
    "\n",
    "EX = rdflib.Namespace(\"http://www.example.org/\")\n",
    "\n",
    "class Meal: ...\n",
    "\n",
    "class Garbage: ...\n",
    "\n",
    "@pwf.as_function_node(\"pizza\")\n",
    "def prepare_pizza() -> u(Meal, uri=EX.Pizza):\n",
    "    return Meal()\n",
    "\n",
    "@pwf.as_function_node(\"unidentified_meal\")\n",
    "def prepare_non_ontological_meal() -> Meal:\n",
    "    return Meal()\n",
    "\n",
    "@pwf.as_function_node(\"rice\")\n",
    "def prepare_rice() -> u(Meal, uri=EX.Rice):\n",
    "    return Meal()\n",
    "\n",
    "@pwf.as_function_node(\"garbage\")\n",
    "def prepare_garbage() -> u(Garbage, uri=EX.Garbage):\n",
    "    return Garbage()\n",
    "\n",
    "@pwf.as_function_node(\"garbage\")\n",
    "def prepare_unhinted_garbage():\n",
    "    return Garbage()\n",
    "\n",
    "@pwf.as_function_node(\"verdict\")\n",
    "def eat(meal: u(Meal, uri=EX.Meal)) -> str:\n",
    "    return f\"Yummy {meal.__class__.__name__} meal\"\n",
    "\n",
    "@pwf.as_function_node(\"verdict\")\n",
    "def eat_pizza(meal: u(Meal, uri=EX.Pizza)) -> str:\n",
    "    return f\"Yummy {meal.__class__.__name__} pizza\""
   ],
   "id": "996ce3d6152f1beb",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Both fully hinted\n",
    "\n",
    "Works fine"
   ],
   "id": "cf1e5e273013acb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.496995Z",
     "start_time": "2025-08-15T22:28:33.383905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"ontoflow\")\n",
    "wf.make = prepare_pizza()\n",
    "wf.eat = eat_pizza(wf.make)\n",
    "wf()"
   ],
   "id": "807c93a2f4436b4f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eat__verdict': 'Yummy Meal pizza'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Upstream type hint is missing\n",
    "\n",
    "Standard `pyiron_workflow` typing behaviour: we are allowed to form the connection (since the source has no hint), but at runtime, we will fail when we try to actually assign the value"
   ],
   "id": "b8e91ece9a45bf7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.506484Z",
     "start_time": "2025-08-15T22:28:33.501537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"no_type\")\n",
    "wf.make = prepare_unhinted_garbage()\n",
    "wf.eat = eat_pizza(wf.make)\n",
    "try:\n",
    "    wf()\n",
    "except FailedChildError as e:\n",
    "    print(e)"
   ],
   "id": "3fde7d3acb113d85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/no_type encountered error in child: {'/no_type/eat.accumulate_and_run': TypeError(\"The channel /no_type/eat.meal cannot take the value `<__main__.Garbage object at 0x11fd86f60>` (<class '__main__.Garbage'>) because it is not compliant with the type hint typing.Annotated[__main__.Meal, ('uri', rdflib.term.URIRef('http://www.example.org/Pizza'))]\")}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Upstream type hint is wrong\n",
    "\n",
    "Standard `pyiron_workflow` typing behaviour: we're not even allowed to form the connection -- the recipe would be invalid"
   ],
   "id": "9af06c9ae9a7512e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.516360Z",
     "start_time": "2025-08-15T22:28:33.513037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"no_type\")\n",
    "wf.make = prepare_garbage()\n",
    "try:\n",
    "    wf.eat = eat_pizza(wf.make)\n",
    "except ChannelConnectionError as e:\n",
    "    print(e)"
   ],
   "id": "a530586921463886",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The channel /no_type/make.garbage (<class 'pyiron_workflow.mixin.injection.OutputDataWithInjection'>) has the correct type (<class 'pyiron_workflow.channels.OutputData'>) to connect with /eat_pizza.meal (<class 'pyiron_workflow.channels.InputData'>), but is not a valid connection.Please check type hints, etc. /no_type/make.garbage.type_hint = typing.Annotated[__main__.Garbage, ('uri', rdflib.term.URIRef('http://www.example.org/Garbage'))]; /eat_pizza.meal.type_hint = typing.Annotated[__main__.Meal, ('uri', rdflib.term.URIRef('http://www.example.org/Pizza'))]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So far, so good: `u` decoration has no negative impact on the existing type hint checking procedures",
   "id": "5995ff2b2e009aee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Upstream ontological hint is missing\n",
    "\n",
    "New ontological behaviour: As with type hints, if one side is missing we just let things pass. Unlike type hints, we can also _execute_ the workflow, because the ontologies only impact the recipe-level behaviour, not the instance behaviour!"
   ],
   "id": "ca34d53309189d96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.524151Z",
     "start_time": "2025-08-15T22:28:33.520527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"no_ontology\")\n",
    "wf.make = prepare_non_ontological_meal()\n",
    "wf.eat = eat_pizza(wf.make)\n",
    "wf()"
   ],
   "id": "d6d324aff20c4e63",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eat__verdict': 'Yummy Meal pizza'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Upstream ontological hint is WRONG\n",
    "\n",
    "New ontological behaviour: new ontological type checking now prevents us from even forming the ontologically invalid connection!"
   ],
   "id": "87f8ae15183930fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.567922Z",
     "start_time": "2025-08-15T22:28:33.528321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"failed_ontology\")\n",
    "wf.make = prepare_rice()\n",
    "try:\n",
    "    wf.eat = eat_pizza(wf.make)\n",
    "except ChannelConnectionError as e:\n",
    "    print(e)"
   ],
   "id": "316fea01813d2c64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The channel /failed_ontology/make.rice (<class 'pyiron_workflow.mixin.injection.OutputDataWithInjection'>) has the correct type (<class 'pyiron_workflow.channels.OutputData'>) to connect with /eat_pizza.meal (<class 'pyiron_workflow.channels.InputData'>), but is not a valid connection.Please check type hints, etc. /failed_ontology/make.rice.type_hint = typing.Annotated[__main__.Meal, ('uri', rdflib.term.URIRef('http://www.example.org/Rice'))]; /eat_pizza.meal.type_hint = typing.Annotated[__main__.Meal, ('uri', rdflib.term.URIRef('http://www.example.org/Pizza'))]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Downstream ontological hint is less specific\n",
    "\n",
    "This should work fine..."
   ],
   "id": "5b883e1acf6aa7c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.616566Z",
     "start_time": "2025-08-15T22:28:33.574427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"relaxed_ontology\")\n",
    "wf.make = prepare_rice()\n",
    "try:\n",
    "    wf.eat = eat(wf.make)\n",
    "except ChannelConnectionError as e:\n",
    "    print(e)"
   ],
   "id": "230dba9264a053e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The channel /relaxed_ontology/make.rice (<class 'pyiron_workflow.mixin.injection.OutputDataWithInjection'>) has the correct type (<class 'pyiron_workflow.channels.OutputData'>) to connect with /eat.meal (<class 'pyiron_workflow.channels.InputData'>), but is not a valid connection.Please check type hints, etc. /relaxed_ontology/make.rice.type_hint = typing.Annotated[__main__.Meal, ('uri', rdflib.term.URIRef('http://www.example.org/Rice'))]; /eat.meal.type_hint = typing.Annotated[__main__.Meal, ('uri', rdflib.term.URIRef('http://www.example.org/Meal'))]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "But! We forgot something! This form of failure is known from the `semantikon` notebook whence these demonstration workflow spring: we never informed the ontology that \"rice\" is a subclass of \"meal\"!\n",
    "\n",
    "We let the ontology know this by adding the corresponding triple to our `rdflib.Graph`. In `pyiron_workflow` we can manage this by pre-populating a `knowledge: rdflib.Graph` property on the graph root (i.e. top-most object) as follows:"
   ],
   "id": "9a332e6a04dc2bd2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.694451Z",
     "start_time": "2025-08-15T22:28:33.623288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"relaxed_ontology\")\n",
    "\n",
    "wf.knowledge = rdflib.Graph()\n",
    "wf.knowledge.add((EX.Rice, rdflib.RDFS.subClassOf, EX.Meal))\n",
    "\n",
    "wf.make = prepare_rice()\n",
    "wf.eat = eat(wf.make)\n",
    "wf()"
   ],
   "id": "cb2a13e6144e6378",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eat__verdict': 'Yummy Meal meal'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ontological triples\n",
    "\n",
    "Alright, for our simple pizza example things are working beautifully. Let's try it with the clothes example."
   ],
   "id": "574ac2f3813504ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.708691Z",
     "start_time": "2025-08-15T22:28:33.702571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rdflib import OWL, Namespace\n",
    "\n",
    "import pyiron_workflow as pwf\n",
    "from pyiron_workflow import knowledge\n",
    "from semantikon.metadata import u\n",
    "from semantikon import ontology\n",
    "\n",
    "EX = Namespace(\"http://www.example.org/\")\n",
    "\n",
    "class Clothes:\n",
    "    pass\n",
    "\n",
    "@pwf.as_function_node\n",
    "def wash(clothes: Clothes) -> u(\n",
    "    Clothes,\n",
    "    triples=(EX.hasProperty, EX.cleaned),\n",
    "    derived_from=\"inputs.clothes\"\n",
    "):\n",
    "    ...\n",
    "    return clothes\n",
    "\n",
    "@pwf.as_function_node\n",
    "def dye(clothes: Clothes, color=\"blue\") -> u(\n",
    "    Clothes,\n",
    "    triples=(EX.hasProperty, EX.color),\n",
    "    derived_from=\"inputs.clothes\",\n",
    "):\n",
    "    ...\n",
    "    return clothes\n",
    "\n",
    "@pwf.as_function_node\n",
    "def sell(\n",
    "    clothes: u(\n",
    "        Clothes, restrictions=(\n",
    "            ((OWL.onProperty, EX.hasProperty), (OWL.someValuesFrom, EX.cleaned)),\n",
    "            ((OWL.onProperty, EX.hasProperty), (OWL.someValuesFrom, EX.color))\n",
    "        )\n",
    "    )\n",
    ") -> int:\n",
    "    price = 10\n",
    "    return price"
   ],
   "id": "dceaab6f57226f07",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.833797Z",
     "start_time": "2025-08-15T22:28:33.712025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_correct_wf = pwf.Workflow(\"my_correct_workflow\")\n",
    "my_correct_wf.dyed_clothes = dye(Clothes())\n",
    "my_correct_wf.washed_clothes = wash(my_correct_wf.dyed_clothes)\n",
    "my_correct_wf.money = sell(my_correct_wf.washed_clothes)\n",
    "my_correct_wf()\n",
    "# Why does it think the top level input needs the properties already???\n",
    "# This was working fine when I generate the graph and validate it after all is said and done..."
   ],
   "id": "c163d51cd2c676c2",
   "outputs": [
    {
     "ename": "ChannelConnectionError",
     "evalue": "The channel /my_correct_workflow/washed_clothes.clothes (<class 'pyiron_workflow.mixin.injection.OutputDataWithInjection'>) has the correct type (<class 'pyiron_workflow.channels.OutputData'>) to connect with /sell.clothes (<class 'pyiron_workflow.channels.InputData'>), but is not a valid connection.Please check type hints, etc. /my_correct_workflow/washed_clothes.clothes.type_hint = typing.Annotated[__main__.Clothes, ('triples', (rdflib.term.URIRef('http://www.example.org/hasProperty'), rdflib.term.URIRef('http://www.example.org/cleaned')), 'derived_from', 'inputs.clothes')]; /sell.clothes.type_hint = typing.Annotated[__main__.Clothes, ('restrictions', (((rdflib.term.URIRef('http://www.w3.org/2002/07/owl#onProperty'), rdflib.term.URIRef('http://www.example.org/hasProperty')), (rdflib.term.URIRef('http://www.w3.org/2002/07/owl#someValuesFrom'), rdflib.term.URIRef('http://www.example.org/cleaned'))), ((rdflib.term.URIRef('http://www.w3.org/2002/07/owl#onProperty'), rdflib.term.URIRef('http://www.example.org/hasProperty')), (rdflib.term.URIRef('http://www.w3.org/2002/07/owl#someValuesFrom'), rdflib.term.URIRef('http://www.example.org/color')))))]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mChannelConnectionError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m my_correct_wf\u001B[38;5;241m.\u001B[39mdyed_clothes \u001B[38;5;241m=\u001B[39m dye(Clothes())\n\u001B[1;32m      3\u001B[0m my_correct_wf\u001B[38;5;241m.\u001B[39mwashed_clothes \u001B[38;5;241m=\u001B[39m wash(my_correct_wf\u001B[38;5;241m.\u001B[39mdyed_clothes)\n\u001B[0;32m----> 4\u001B[0m my_correct_wf\u001B[38;5;241m.\u001B[39mmoney \u001B[38;5;241m=\u001B[39m \u001B[43msell\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmy_correct_wf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwashed_clothes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m my_correct_wf()\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Why does it think the top level input needs the properties already???\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# This was working fine when I generate the graph and validate it after all is said and done...\u001B[39;00m\n",
      "File \u001B[0;32m~/dev/pyiron/pyiron_workflow/pyiron_workflow/node.py:190\u001B[0m, in \u001B[0;36mNode.__init__\u001B[0;34m(self, label, parent, delete_existing_savefiles, autoload, autorun, checkpoint, *args, **kwargs)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;66;03m# A place for power-users to bypass node-injection\u001B[39;00m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_setup_node()\n\u001B[0;32m--> 190\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_after_node_setup\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    191\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    192\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelete_existing_savefiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdelete_existing_savefiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    193\u001B[0m \u001B[43m    \u001B[49m\u001B[43mautoload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mautoload\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    194\u001B[0m \u001B[43m    \u001B[49m\u001B[43mautorun\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mautorun\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    195\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/dev/pyiron/pyiron_workflow/pyiron_workflow/node.py:228\u001B[0m, in \u001B[0;36mNode._after_node_setup\u001B[0;34m(self, delete_existing_savefiles, autoload, autorun, *args, **kwargs)\u001B[0m\n\u001B[1;32m    225\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload(backend\u001B[38;5;241m=\u001B[39mautoload)\n\u001B[1;32m    226\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m--> 228\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_input_values\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m autorun:\n\u001B[1;32m    231\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39msuppress(ReadinessError):\n",
      "File \u001B[0;32m~/dev/pyiron/pyiron_workflow/pyiron_workflow/node.py:399\u001B[0m, in \u001B[0;36mNode.set_input_values\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    382\u001B[0m \u001B[38;5;124;03mMatch keywords to input channels and update their values.\u001B[39;00m\n\u001B[1;32m    383\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    396\u001B[0m \u001B[38;5;124;03m        labels.\u001B[39;00m\n\u001B[1;32m    397\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_complete_input(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m--> 399\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m v\n",
      "File \u001B[0;32m~/dev/pyiron/pyiron_workflow/pyiron_workflow/io.py:115\u001B[0m, in \u001B[0;36mIO.__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__setitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key: \u001B[38;5;28mstr\u001B[39m, value: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 115\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__setattr__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/dev/pyiron/pyiron_workflow/pyiron_workflow/io.py:88\u001B[0m, in \u001B[0;36mIO.__setattr__\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__setattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key: \u001B[38;5;28mstr\u001B[39m, value: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchannel_dict:\n\u001B[0;32m---> 88\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_assign_value_to_existing_channel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchannel_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_channel_class):\n\u001B[1;32m     90\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;241m!=\u001B[39m value\u001B[38;5;241m.\u001B[39mlabel:\n",
      "File \u001B[0;32m~/dev/pyiron/pyiron_workflow/pyiron_workflow/io.py:104\u001B[0m, in \u001B[0;36mIO._assign_value_to_existing_channel\u001B[0;34m(self, channel, value)\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_assign_value_to_existing_channel\u001B[39m(\u001B[38;5;28mself\u001B[39m, channel: OwnedType, value: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    103\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, HasChannel):\n\u001B[0;32m--> 104\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_assign_a_channel_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchannel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    105\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    106\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assign_a_non_channel_value(channel, value)\n",
      "File \u001B[0;32m~/dev/pyiron/pyiron_workflow/pyiron_workflow/io.py:225\u001B[0m, in \u001B[0;36mInputs._assign_a_channel_value\u001B[0;34m(self, channel, value)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_assign_a_channel_value\u001B[39m(\u001B[38;5;28mself\u001B[39m, channel: OwnedType, value: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;66;03m# Allow the owned input data channel to overwrite its connection\u001B[39;00m\n\u001B[1;32m    224\u001B[0m     channel\u001B[38;5;241m.\u001B[39mdisconnect_all()\n\u001B[0;32m--> 225\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_assign_a_channel_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchannel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/dev/pyiron/pyiron_workflow/pyiron_workflow/io.py:109\u001B[0m, in \u001B[0;36mIO._assign_a_channel_value\u001B[0;34m(self, channel, value)\u001B[0m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_assign_a_channel_value\u001B[39m(\u001B[38;5;28mself\u001B[39m, channel: OwnedType, value: HasChannel) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 109\u001B[0m     \u001B[43mchannel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchannel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/dev/pyiron/pyiron_workflow/pyiron_workflow/channels.py:173\u001B[0m, in \u001B[0;36mChannel.connect\u001B[0;34m(self, *others)\u001B[0m\n\u001B[1;32m    171\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    172\u001B[0m         wrong_parents_callback\u001B[38;5;241m.\u001B[39mcall()\n\u001B[0;32m--> 173\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ChannelConnectionError(\n\u001B[1;32m    174\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connection_conjugate_failure_message(other)\n\u001B[1;32m    175\u001B[0m         ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    177\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    178\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan only connect to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconnection_conjugate()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    179\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobjects, but \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfull_label\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    180\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgot \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mother\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(other)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    181\u001B[0m     )\n",
      "\u001B[0;31mChannelConnectionError\u001B[0m: The channel /my_correct_workflow/washed_clothes.clothes (<class 'pyiron_workflow.mixin.injection.OutputDataWithInjection'>) has the correct type (<class 'pyiron_workflow.channels.OutputData'>) to connect with /sell.clothes (<class 'pyiron_workflow.channels.InputData'>), but is not a valid connection.Please check type hints, etc. /my_correct_workflow/washed_clothes.clothes.type_hint = typing.Annotated[__main__.Clothes, ('triples', (rdflib.term.URIRef('http://www.example.org/hasProperty'), rdflib.term.URIRef('http://www.example.org/cleaned')), 'derived_from', 'inputs.clothes')]; /sell.clothes.type_hint = typing.Annotated[__main__.Clothes, ('restrictions', (((rdflib.term.URIRef('http://www.w3.org/2002/07/owl#onProperty'), rdflib.term.URIRef('http://www.example.org/hasProperty')), (rdflib.term.URIRef('http://www.w3.org/2002/07/owl#someValuesFrom'), rdflib.term.URIRef('http://www.example.org/cleaned'))), ((rdflib.term.URIRef('http://www.w3.org/2002/07/owl#onProperty'), rdflib.term.URIRef('http://www.example.org/hasProperty')), (rdflib.term.URIRef('http://www.w3.org/2002/07/owl#someValuesFrom'), rdflib.term.URIRef('http://www.example.org/color')))))]"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.838481Z",
     "start_time": "2025-08-15T18:28:25.544501Z"
    }
   },
   "cell_type": "code",
   "source": "my_correct_wf.money.inputs.clothes.type_hint",
   "id": "ea5f7f75e6117294",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Could not find attribute 'money' on my_correct_workflow (Workflow) or among its children (dict_keys(['dyed_clothes', 'washed_clothes'])).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/dev/pyiron/pyiron_workflow/pyiron_workflow/mixin/lexical.py:286\u001B[0m, in \u001B[0;36mLexicalParent.__getattr__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 286\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_children\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m key_error:\n\u001B[1;32m    288\u001B[0m     \u001B[38;5;66;03m# Raise an attribute error from getattr to make sure hasattr works well!\u001B[39;00m\n",
      "File \u001B[0;32m~/dev/miniforge3/envs/pyiron12/lib/python3.12/site-packages/bidict/_base.py:524\u001B[0m, in \u001B[0;36mBidictBase.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    523\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"*x.__getitem__(key) âŸº x[key]*\"\"\"\u001B[39;00m\n\u001B[0;32m--> 524\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fwdm\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'money'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmy_correct_wf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmoney\u001B[49m\u001B[38;5;241m.\u001B[39minputs\u001B[38;5;241m.\u001B[39mclothes\u001B[38;5;241m.\u001B[39mtype_hint\n",
      "File \u001B[0;32m~/dev/pyiron/pyiron_workflow/pyiron_workflow/mixin/lexical.py:295\u001B[0m, in \u001B[0;36mLexicalParent.__getattr__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(matches) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    294\u001B[0m     msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Did you mean \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmatches[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and not \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 295\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkey_error\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: Could not find attribute 'money' on my_correct_workflow (Workflow) or among its children (dict_keys(['dyed_clothes', 'washed_clothes']))."
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.838606Z",
     "start_time": "2025-08-15T18:28:27.786763Z"
    }
   },
   "cell_type": "code",
   "source": "ontology.validate_values(knowledge.parse_workflow(my_correct_wf))",
   "id": "e77cd82add13ad56",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_triples': [], 'incompatible_connections': []}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@pwf.as_macro_node\n",
    "def my_correct_macro(self, clothes: Clothes):\n",
    "    self.dyed_clothes = dye(clothes)\n",
    "    self.washed_clothes = wash(self.dyed_clothes)\n",
    "    self.money = sell(self.washed_clothes)\n",
    "    return self.money\n",
    "\n",
    "correct_m = my_correct_macro(Clothes())\n",
    "correct_m()"
   ],
   "id": "f6b77dff8a1e5b84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.840364Z",
     "start_time": "2025-08-14T17:57:38.732020Z"
    }
   },
   "cell_type": "code",
   "source": "ontology.validate_values(knowledge.parse_workflow(correct_m))",
   "id": "e319f2ea6db43477",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_triples': [], 'incompatible_connections': []}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.840483Z",
     "start_time": "2025-08-14T17:57:39.163014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_wrong_wf = pwf.Workflow(\"my_wrong_workflow\")\n",
    "my_wrong_wf.washed_clothes = wash(Clothes())\n",
    "my_wrong_wf.money = sell(my_wrong_wf.washed_clothes)\n",
    "my_wrong_wf()"
   ],
   "id": "ef51d73b476d6a0b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'money__10': 10}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.840578Z",
     "start_time": "2025-08-14T17:57:39.471881Z"
    }
   },
   "cell_type": "code",
   "source": "ontology.validate_values(knowledge.parse_workflow(my_wrong_wf))",
   "id": "d8b45bde3eda7aa9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_triples': [(rdflib.term.URIRef('my_wrong_workflow.money.inputs.clothes'),\n",
       "   rdflib.term.URIRef('http://www.example.org/hasProperty'),\n",
       "   rdflib.term.URIRef('http://www.example.org/color'))],\n",
       " 'incompatible_connections': []}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.840862Z",
     "start_time": "2025-08-14T17:57:39.811107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@pwf.as_macro_node\n",
    "def my_wrong_macro(self, clothes: Clothes):\n",
    "    self.washed_clothes = wash(clothes)\n",
    "    self.money = sell(self.washed_clothes)\n",
    "    return self.money\n",
    "\n",
    "wrong_m = my_wrong_macro(Clothes())\n",
    "wrong_m()"
   ],
   "id": "1520f248cef9296e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'money': 10}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.841021Z",
     "start_time": "2025-08-14T17:57:40.225949Z"
    }
   },
   "cell_type": "code",
   "source": "ontology.validate_values(knowledge.parse_workflow(wrong_m))",
   "id": "b59cfa8b01752473",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_triples': [(rdflib.term.URIRef('my_wrong_macro.money.inputs.clothes'),\n",
       "   rdflib.term.URIRef('http://www.example.org/hasProperty'),\n",
       "   rdflib.term.URIRef('http://www.example.org/color'))],\n",
       " 'incompatible_connections': []}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.841117Z",
     "start_time": "2025-08-14T17:57:40.676923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@pwf.as_function_node\n",
    "def dye_with_cancel(clothes: Clothes, color=\"blue\") -> u(\n",
    "    Clothes,\n",
    "    triples=(EX.hasProperty, EX.color),\n",
    "    derived_from=\"inputs.clothes\",\n",
    "    cancel=(EX.hasProperty, EX.cleaned)\n",
    "):\n",
    "    return clothes"
   ],
   "id": "3055fbc547280d91",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.841184Z",
     "start_time": "2025-08-14T17:57:41.024260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_wf_with_wrong_order = pwf.Workflow(\"my_workflow_with_wrong_order\")\n",
    "my_wf_with_wrong_order.washed_clothes = wash(Clothes())\n",
    "my_wf_with_wrong_order.dyed_clothes = dye_with_cancel(my_wf_with_wrong_order.washed_clothes)\n",
    "my_wf_with_wrong_order.money = sell(my_wf_with_wrong_order.dyed_clothes)\n",
    "my_wf_with_wrong_order()"
   ],
   "id": "912ab5035ea69c03",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'money__10': 10}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:33.841248Z",
     "start_time": "2025-08-14T17:57:41.482159Z"
    }
   },
   "cell_type": "code",
   "source": "ontology.validate_values(knowledge.parse_workflow(my_wf_with_wrong_order))",
   "id": "8e590c0187917feb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_triples': [(rdflib.term.URIRef('my_workflow_with_wrong_order.money.inputs.clothes'),\n",
       "   rdflib.term.URIRef('http://www.example.org/hasProperty'),\n",
       "   rdflib.term.URIRef('http://www.example.org/cleaned'))],\n",
       " 'incompatible_connections': []}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This example produces expected outcomes the whole way through.\n",
    "\n",
    "As a first stage then, we can internally create type-validated connections, search for a graph root, transform the entire workflow graph to a \"parsed\" graph, feed that to semantikon, and reverse the connection iff we encounter a problem.\n",
    "This is not the most computationally efficient approach, but should be pretty robust and very fast to implement."
   ],
   "id": "ed2b6333d5c3be36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Node suggestions\n",
    "\n",
    "Given a hinted channel instance in the context of some workflow, we can ask for suggestions of sibling (node, channel) to connect to:"
   ],
   "id": "c612162f836d4c76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T22:28:39.391858Z",
     "start_time": "2025-08-15T22:28:39.349215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"ontoflow\")\n",
    "wf.make = prepare_pizza()\n",
    "wf.eat = eat_pizza()\n",
    "suggest.suggest_connections(wf.eat.inputs.meal)"
   ],
   "id": "7e01728625255664",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at make\n",
      "outputs OutputsWithInjection ['pizza']\n",
      "What about make__pizza\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(<__main__.prepare_pizza at 0x11fe022a0>,\n",
       "  <pyiron_workflow.mixin.injection.OutputDataWithInjection at 0x11fe02030>)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Known Issues\n",
    "\n",
    "- This implementation naively creates a circular dependence: the `channels` module needs the `knowledge` module to evaluate the ontological validity of new connections, but the `knowledge` module relies on `workflow` and `nodes.composite` to parse graphs, and these in turn depend on `channels`. For now, we avoid dealing with this by importing `knowledge` locally in `channels` when it's time to use the ontology.\n",
    "- There are strings everywhere. The ontological features rely heavily on dictionaries, which are tough to type check and rely on string-based key access. E.g., when we want to see if the ontological validation raised any errors, we need to manually check on two dictionary entries by name. This is fragile.\n",
    "- It is heinously inefficient. At every new ontologically-hinted connection, we reconstruct the entire recipe dictionary before positing the new connection and checking its validity. I'm not sure we'll get around validation operating on the entire graph, but we should adjust `pyiron_workflow` to store more recipe information at the class level where it is statically known (macros, function nodes, etc)\n",
    "- This is not _at all_ edge-case tested. This works for the special cases we test here, and there's no generic guarantee this functionality works for more complex ontologies or more complex workflows.\n",
    "\n",
    "# Open Questions\n",
    "\n",
    "- What will happen when we more deeply nest workflows?\n",
    "- Where and how will things break when we introduce other node types, i.e. for- and while-loops, dataclass nodes, or other transformers?"
   ],
   "id": "c1e37915be9898fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7f6ff0736bd3c87c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
