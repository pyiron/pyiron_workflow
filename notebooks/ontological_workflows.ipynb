{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ontological validation in `pyiron_workflow`\n",
    "\n",
    "In `pyiron_workflow`, we leverage pyiron's ontological validation of workflow graphs found in `semantikon`. We strive to support every syntactic aspect of `semantikon`, which you can read more about on [the GitHub repository](https://github.com/pyiron/semantikon) or [the docs]() for that project.\n",
    "\n",
    "This is accomplished practically by wrapping function IO (or dataclass nodes) with the `semantikon.metadata.u` call. Here, we demonstrate a variety of use-cases, and show how this functionality can be taken further to get data type- and ontologically-valid suggestions for new connections in your workflow, or new nodes to add to it.\n",
    "\n",
    "Note that ontological typing _only_ works when there's a parent object around (a macro or a workflow)."
   ],
   "id": "78e57715fcc8beea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ontological connection checking\n",
    "\n",
    "Some success and failure cases in the presence or absence of ontological hints. A key takeway is that these hints function very much like `pyiron_workflow`'s regular type hinting: if hints are present on both sides of a new connection, they _must_ be valid, but if one or both sides are missing the hint we skip the validation."
   ],
   "id": "8b0b5e89a462c4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:55.850601Z",
     "start_time": "2025-09-12T15:17:55.840634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import rdflib\n",
    "\n",
    "from semantikon.metadata import u\n",
    "\n",
    "import pyiron_workflow as pwf\n",
    "from pyiron_workflow import suggest\n",
    "from pyiron_workflow.channels import ChannelConnectionError\n",
    "from pyiron_workflow.nodes.composite import FailedChildError\n",
    "\n",
    "\n",
    "EX = rdflib.Namespace(\"http://www.example.org/\")\n",
    "\n",
    "class Meal: ...\n",
    "\n",
    "class Garbage: ...\n",
    "\n",
    "@pwf.as_function_node(\"pizza\")\n",
    "def prepare_pizza() -> u(Meal, uri=EX.Pizza):\n",
    "    return Meal()\n",
    "\n",
    "@pwf.as_function_node(\"unidentified_meal\")\n",
    "def prepare_non_ontological_meal() -> Meal:\n",
    "    return Meal()\n",
    "\n",
    "@pwf.as_function_node(\"rice\")\n",
    "def prepare_rice() -> u(Meal, uri=EX.Rice):\n",
    "    return Meal()\n",
    "\n",
    "@pwf.as_function_node(\"garbage\")\n",
    "def prepare_garbage() -> u(Garbage, uri=EX.Garbage):\n",
    "    return Garbage()\n",
    "\n",
    "@pwf.as_function_node(\"garbage\")\n",
    "def prepare_unhinted_garbage():\n",
    "    return Garbage()\n",
    "\n",
    "@pwf.as_function_node(\"verdict\")\n",
    "def eat(meal: u(Meal, uri=EX.Meal)) -> str:\n",
    "    return f\"Yummy {meal.__class__.__name__} meal\"\n",
    "\n",
    "@pwf.as_function_node(\"verdict\")\n",
    "def eat_pizza(meal: u(Meal, uri=EX.Pizza)) -> str:\n",
    "    return f\"Yummy {meal.__class__.__name__} pizza\""
   ],
   "id": "996ce3d6152f1beb",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Both fully hinted\n",
    "\n",
    "Works fine"
   ],
   "id": "cf1e5e273013acb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:55.990573Z",
     "start_time": "2025-09-12T15:17:55.855512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"ontoflow\")\n",
    "wf.make = prepare_pizza()\n",
    "wf.eat = eat_pizza(wf.make)\n",
    "wf()"
   ],
   "id": "807c93a2f4436b4f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eat__verdict': 'Yummy Meal pizza'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Upstream type hint is missing\n",
    "\n",
    "Standard `pyiron_workflow` typing behaviour: we are allowed to form the connection (since the source has no hint), but at runtime, we will fail when we try to actually assign the value"
   ],
   "id": "b8e91ece9a45bf7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:55.998798Z",
     "start_time": "2025-09-12T15:17:55.995091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"no_type\")\n",
    "wf.make = prepare_unhinted_garbage()\n",
    "wf.eat = eat_pizza(wf.make)\n",
    "try:\n",
    "    wf.recovery = None\n",
    "    wf()\n",
    "except FailedChildError as e:\n",
    "    print(e)"
   ],
   "id": "3fde7d3acb113d85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/no_type encountered error in child: {'/no_type/eat.accumulate_and_run': TypeError(\"The channel /no_type/eat.meal cannot take the value `<__main__.Garbage object at 0x132838950>` (<class '__main__.Garbage'>) because it is not compliant with the type hint typing.Annotated[__main__.Meal, ('uri', rdflib.term.URIRef('http://www.example.org/Pizza'))]\")}\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Upstream type hint is wrong\n",
    "\n",
    "Standard `pyiron_workflow` typing behaviour: we're not even allowed to form the connection -- the recipe would be invalid"
   ],
   "id": "9af06c9ae9a7512e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:56.024027Z",
     "start_time": "2025-09-12T15:17:56.020676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"no_type\")\n",
    "wf.make = prepare_garbage()\n",
    "try:\n",
    "    wf.eat = eat_pizza(wf.make)\n",
    "except ChannelConnectionError as e:\n",
    "    print(e)"
   ],
   "id": "a530586921463886",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The upstream channel /no_type/make.garbage cannot connect to the downstream channel /no_type/eat_pizza.meal because the upstream type hint (typing.Annotated[__main__.Garbage, ('uri', rdflib.term.URIRef('http://www.example.org/Garbage'))]) is not as or more specific than the downstream type hint (typing.Annotated[__main__.Meal, ('uri', rdflib.term.URIRef('http://www.example.org/Pizza'))]).\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So far, so good: `u` decoration has no negative impact on the existing type hint checking procedures",
   "id": "5995ff2b2e009aee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Upstream ontological hint is missing\n",
    "\n",
    "New ontological behaviour: As with type hints, if one side is missing we just let things pass. Unlike type hints, we can also _execute_ the workflow, because the ontologies only impact the recipe-level behaviour, not the instance behaviour!"
   ],
   "id": "ca34d53309189d96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:56.040728Z",
     "start_time": "2025-09-12T15:17:56.036621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"no_ontology\")\n",
    "wf.make = prepare_non_ontological_meal()\n",
    "wf.eat = eat_pizza(wf.make)\n",
    "wf()"
   ],
   "id": "d6d324aff20c4e63",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eat__verdict': 'Yummy Meal pizza'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Upstream ontological hint is WRONG\n",
    "\n",
    "New ontological behaviour: new ontological type checking now prevents us from even forming the ontologically invalid connection!"
   ],
   "id": "87f8ae15183930fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:56.125043Z",
     "start_time": "2025-09-12T15:17:56.057272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"failed_ontology\")\n",
    "wf.make = prepare_rice()\n",
    "try:\n",
    "    wf.eat = eat_pizza(wf.make)\n",
    "except ChannelConnectionError as e:\n",
    "    print(e)"
   ],
   "id": "316fea01813d2c64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The upstream channel /failed_ontology/make.rice cannot connect to the downstream channel /failed_ontology/eat_pizza.meal because the upstream type hint (typing.Annotated[__main__.Meal, ('uri', rdflib.term.URIRef('http://www.example.org/Rice'))]) and downstream type hint (typing.Annotated[__main__.Meal, ('uri', rdflib.term.URIRef('http://www.example.org/Pizza'))]) produce a non-empty ontological validation report:\n",
      "{'missing_triples': [], 'incompatible_connections': [(rdflib.term.URIRef('failed_ontology.eat_pizza.inputs.meal'), rdflib.term.URIRef('failed_ontology.make.outputs.rice'), [rdflib.term.URIRef('http://www.example.org/Pizza')], [rdflib.term.URIRef('http://www.example.org/Rice')])], 'distinct_units': {}}\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Downstream ontological hint is less specific\n",
    "\n",
    "This should work fine..."
   ],
   "id": "5b883e1acf6aa7c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:56.209281Z",
     "start_time": "2025-09-12T15:17:56.142961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"relaxed_ontology\")\n",
    "wf.make = prepare_rice()\n",
    "try:\n",
    "    wf.eat = eat(wf.make)\n",
    "except ChannelConnectionError as e:\n",
    "    print(e)"
   ],
   "id": "230dba9264a053e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The upstream channel /relaxed_ontology/make.rice cannot connect to the downstream channel /relaxed_ontology/eat.meal because the upstream type hint (typing.Annotated[__main__.Meal, ('uri', rdflib.term.URIRef('http://www.example.org/Rice'))]) and downstream type hint (typing.Annotated[__main__.Meal, ('uri', rdflib.term.URIRef('http://www.example.org/Meal'))]) produce a non-empty ontological validation report:\n",
      "{'missing_triples': [], 'incompatible_connections': [(rdflib.term.URIRef('relaxed_ontology.eat.inputs.meal'), rdflib.term.URIRef('relaxed_ontology.make.outputs.rice'), [rdflib.term.URIRef('http://www.example.org/Meal')], [rdflib.term.URIRef('http://www.example.org/Rice')])], 'distinct_units': {}}\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "But! We forgot something! This form of failure is known from the `semantikon` notebook whence these demonstration workflow spring: we never informed the ontology that \"rice\" is a subclass of \"meal\"!\n",
    "\n",
    "We let the ontology know this by adding the corresponding triple to our `rdflib.Graph`. In `pyiron_workflow` we can manage this by pre-populating a `knowledge: rdflib.Graph` property on the graph root (i.e. top-most object) as follows:"
   ],
   "id": "9a332e6a04dc2bd2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:56.332796Z",
     "start_time": "2025-09-12T15:17:56.213505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"relaxed_ontology\")\n",
    "\n",
    "wf.knowledge = rdflib.Graph()\n",
    "wf.knowledge.add((EX.Rice, rdflib.RDFS.subClassOf, EX.Meal))\n",
    "\n",
    "wf.make = prepare_rice()\n",
    "wf.eat = eat(wf.make)\n",
    "wf()"
   ],
   "id": "cb2a13e6144e6378",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eat__verdict': 'Yummy Meal meal'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ontological triples\n",
    "\n",
    "Alright, for our simple pizza example things are working beautifully. Let's try it with the clothes example. Note that the `derived_from` annotation does not cause the `uri` annotation to be inherited, so even when we derive our output from our input, we re-state the URI."
   ],
   "id": "574ac2f3813504ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "69923a637cd8bc00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:56.342201Z",
     "start_time": "2025-09-12T15:17:56.335705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EX = rdflib.Namespace(\"http://www.example.org/\")\n",
    "\n",
    "class Clothes:\n",
    "    pass\n",
    "\n",
    "@pwf.as_function_node\n",
    "def wash(clothes: u(Clothes, uri=EX.Clothes)) -> u(\n",
    "    Clothes,\n",
    "    uri=EX.Clothes,\n",
    "    triples=(EX.hasProperty, EX.cleaned),\n",
    "    derived_from=\"inputs.clothes\"\n",
    "):\n",
    "    ...\n",
    "    return clothes\n",
    "\n",
    "@pwf.as_function_node\n",
    "def dye(clothes: u(Clothes, uri=EX.Clothes), color=\"blue\") -> u(\n",
    "    Clothes,\n",
    "    uri=EX.Clothes,\n",
    "    triples=(EX.hasProperty, EX.color),\n",
    "    derived_from=\"inputs.clothes\",\n",
    "):\n",
    "    ...\n",
    "    return clothes\n",
    "\n",
    "@pwf.as_function_node\n",
    "def sell(\n",
    "    clothes: u(\n",
    "        Clothes,\n",
    "        uri=EX.Clothes,\n",
    "        restrictions=(\n",
    "            ((rdflib.OWL.onProperty, EX.hasProperty), (rdflib.OWL.someValuesFrom, EX.cleaned)),\n",
    "            ((rdflib.OWL.onProperty, EX.hasProperty), (rdflib.OWL.someValuesFrom, EX.color)),\n",
    "        )\n",
    "    )\n",
    ") -> int:\n",
    "    price = 10\n",
    "    return price"
   ],
   "id": "dceaab6f57226f07",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Now with `restrictions`\n",
    "\n",
    "In the base case, everything works fine. The restrictions are correctly parsed.\n",
    "\n",
    "Note that unlike the `semantikon` notebook, here we had to make sure that all the node inputs are also `u` annotated (even if it's just to trivially link the type to its ontology counterpart). This is because type checking only occurs in `pyiron_workflow` when _both_ sides of the connection are typed! We follow this rule for both standard data types and ontological types."
   ],
   "id": "e9bb559e718b4f66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:56.752725Z",
     "start_time": "2025-09-12T15:17:56.349889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_correct_wf = pwf.Workflow(\"my_correct_workflow\")\n",
    "my_correct_wf.dyed_clothes = dye(Clothes())\n",
    "my_correct_wf.washed_clothes = wash(my_correct_wf.dyed_clothes)\n",
    "my_correct_wf.money = sell(my_correct_wf.washed_clothes)\n",
    "my_correct_wf()"
   ],
   "id": "c163d51cd2c676c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'money__price': 10}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## As a macro\n",
    "\n",
    "This also works fine! Be careful though, here we've only demonstrated that it _can_ work for macros, and have not yet guaranteed it works for _all_ macros."
   ],
   "id": "8dd38830d50f1b73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:57.215696Z",
     "start_time": "2025-09-12T15:17:56.759043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@pwf.as_macro_node\n",
    "def my_correct_macro(self, clothes: Clothes):\n",
    "    self.dyed_clothes = dye(clothes)\n",
    "    self.washed_clothes = wash(self.dyed_clothes)\n",
    "    self.money = sell(self.washed_clothes)\n",
    "    return self.money\n",
    "\n",
    "correct_m = my_correct_macro(Clothes())\n",
    "correct_m()"
   ],
   "id": "f6b77dff8a1e5b84",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'money': 10}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Trivial failure\n",
    "\n",
    "If we skip a step, our `sell` `restrictions` are not fulfilled, and we sensibly fail."
   ],
   "id": "c71478fc498bdb30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:57.303813Z",
     "start_time": "2025-09-12T15:17:57.220611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_wrong_wf = pwf.Workflow(\"my_wrong_workflow\")\n",
    "my_wrong_wf.washed_clothes = wash(Clothes())\n",
    "try:\n",
    "    my_wrong_wf.money = sell(my_wrong_wf.washed_clothes)\n",
    "except ChannelConnectionError as e:\n",
    "    print(e)"
   ],
   "id": "ef51d73b476d6a0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The upstream channel /my_wrong_workflow/washed_clothes.clothes cannot connect to the downstream channel /my_wrong_workflow/sell.clothes because the upstream type hint (typing.Annotated[__main__.Clothes, ('uri', rdflib.term.URIRef('http://www.example.org/Clothes'), 'triples', (rdflib.term.URIRef('http://www.example.org/hasProperty'), rdflib.term.URIRef('http://www.example.org/cleaned')), 'derived_from', 'inputs.clothes')]) and downstream type hint (typing.Annotated[__main__.Clothes, ('uri', rdflib.term.URIRef('http://www.example.org/Clothes'), 'restrictions', (((rdflib.term.URIRef('http://www.w3.org/2002/07/owl#onProperty'), rdflib.term.URIRef('http://www.example.org/hasProperty')), (rdflib.term.URIRef('http://www.w3.org/2002/07/owl#someValuesFrom'), rdflib.term.URIRef('http://www.example.org/cleaned'))), ((rdflib.term.URIRef('http://www.w3.org/2002/07/owl#onProperty'), rdflib.term.URIRef('http://www.example.org/hasProperty')), (rdflib.term.URIRef('http://www.w3.org/2002/07/owl#someValuesFrom'), rdflib.term.URIRef('http://www.example.org/color')))))]) produce a non-empty ontological validation report:\n",
      "{'missing_triples': [(rdflib.term.URIRef('my_wrong_workflow.sell.inputs.clothes'), rdflib.term.URIRef('http://www.example.org/hasProperty'), rdflib.term.URIRef('http://www.example.org/color'))], 'incompatible_connections': [], 'distinct_units': {}}\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Macro failure\n",
    "\n",
    "When we wrap the failing code as a macro, we don't fail until we try to instantiate that macro -- that is the first time the recipe code is evaluated and ontologically evaluated, at which point we fail at the connection formation just like in the workflow example.\n",
    "\n",
    "In the future, if we move to `pyiron_workflow` decorators first producing (and validating) `flowrep` recipes and _then_ using these to create `pyiron_workflow` node classes, we'd be able to nicely fail at the macro definition time instead!"
   ],
   "id": "9b3d6a2838caaf72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:57.393224Z",
     "start_time": "2025-09-12T15:17:57.308032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@pwf.as_macro_node\n",
    "def my_wrong_macro(self, clothes: Clothes):\n",
    "    self.washed_clothes = wash(clothes)\n",
    "    self.money = sell(self.washed_clothes)\n",
    "    return self.money\n",
    "\n",
    "try:\n",
    "    my_wrong_macro()\n",
    "except ChannelConnectionError as e:\n",
    "    print(e)"
   ],
   "id": "1520f248cef9296e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The upstream channel /my_wrong_macro/washed_clothes.clothes cannot connect to the downstream channel /my_wrong_macro/sell.clothes because the upstream type hint (typing.Annotated[__main__.Clothes, ('uri', rdflib.term.URIRef('http://www.example.org/Clothes'), 'triples', (rdflib.term.URIRef('http://www.example.org/hasProperty'), rdflib.term.URIRef('http://www.example.org/cleaned')), 'derived_from', 'inputs.clothes')]) and downstream type hint (typing.Annotated[__main__.Clothes, ('uri', rdflib.term.URIRef('http://www.example.org/Clothes'), 'restrictions', (((rdflib.term.URIRef('http://www.w3.org/2002/07/owl#onProperty'), rdflib.term.URIRef('http://www.example.org/hasProperty')), (rdflib.term.URIRef('http://www.w3.org/2002/07/owl#someValuesFrom'), rdflib.term.URIRef('http://www.example.org/cleaned'))), ((rdflib.term.URIRef('http://www.w3.org/2002/07/owl#onProperty'), rdflib.term.URIRef('http://www.example.org/hasProperty')), (rdflib.term.URIRef('http://www.w3.org/2002/07/owl#someValuesFrom'), rdflib.term.URIRef('http://www.example.org/color')))))]) produce a non-empty ontological validation report:\n",
      "{'missing_triples': [(rdflib.term.URIRef('my_wrong_macro.sell.inputs.clothes'), rdflib.term.URIRef('http://www.example.org/hasProperty'), rdflib.term.URIRef('http://www.example.org/color'))], 'incompatible_connections': [], 'distinct_units': {}}\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Complex failure\n",
    "\n",
    "Now let's be a little sneaky -- as usual, our \"dye\" node will add \"color\" to the clothes, but let's leverage our ontological power to _remove_ the \"clean\" state!"
   ],
   "id": "36fb86b51a117df2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:57.399302Z",
     "start_time": "2025-09-12T15:17:57.396390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@pwf.as_function_node\n",
    "def dye_with_cancel(clothes: Clothes, color=\"blue\") -> u(\n",
    "    Clothes,\n",
    "    uri=EX.Clothes,\n",
    "    derived_from=\"inputs.clothes\",\n",
    "    triples=(EX.hasProperty, EX.color),\n",
    "    cancel=(EX.hasProperty, EX.cleaned)\n",
    "):\n",
    "    return clothes"
   ],
   "id": "3055fbc547280d91",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We fail, as expected. The error messages for failed ontology validations are still extremely opaque, but we can see that the upstream node `'cancel'`s the `.../cleaned` property, while the downstream type hint still requires `hasProperty` `cleaned`.",
   "id": "599404a35b15ef6b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:57.496940Z",
     "start_time": "2025-09-12T15:17:57.408824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_wf_with_cancellation = pwf.Workflow(\"my_wf_with_cancellation\")\n",
    "my_wf_with_cancellation.washed_clothes = wash(Clothes())\n",
    "my_wf_with_cancellation.dyed_clothes = dye_with_cancel(my_wf_with_cancellation.washed_clothes)\n",
    "try:\n",
    "    my_wf_with_cancellation.money = sell(my_wf_with_cancellation.dyed_clothes)\n",
    "except ChannelConnectionError as e:\n",
    "    print(e)"
   ],
   "id": "912ab5035ea69c03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The upstream channel /my_wf_with_cancellation/dyed_clothes.clothes cannot connect to the downstream channel /my_wf_with_cancellation/sell.clothes because the upstream type hint (typing.Annotated[__main__.Clothes, ('uri', rdflib.term.URIRef('http://www.example.org/Clothes'), 'triples', (rdflib.term.URIRef('http://www.example.org/hasProperty'), rdflib.term.URIRef('http://www.example.org/color')), 'derived_from', 'inputs.clothes', 'extra', {'cancel': (rdflib.term.URIRef('http://www.example.org/hasProperty'), rdflib.term.URIRef('http://www.example.org/cleaned'))})]) and downstream type hint (typing.Annotated[__main__.Clothes, ('uri', rdflib.term.URIRef('http://www.example.org/Clothes'), 'restrictions', (((rdflib.term.URIRef('http://www.w3.org/2002/07/owl#onProperty'), rdflib.term.URIRef('http://www.example.org/hasProperty')), (rdflib.term.URIRef('http://www.w3.org/2002/07/owl#someValuesFrom'), rdflib.term.URIRef('http://www.example.org/cleaned'))), ((rdflib.term.URIRef('http://www.w3.org/2002/07/owl#onProperty'), rdflib.term.URIRef('http://www.example.org/hasProperty')), (rdflib.term.URIRef('http://www.w3.org/2002/07/owl#someValuesFrom'), rdflib.term.URIRef('http://www.example.org/color')))))]) produce a non-empty ontological validation report:\n",
      "{'missing_triples': [(rdflib.term.URIRef('my_wf_with_cancellation.sell.inputs.clothes'), rdflib.term.URIRef('http://www.example.org/hasProperty'), rdflib.term.URIRef('http://www.example.org/cleaned'))], 'incompatible_connections': [], 'distinct_units': {}}\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# It's alpha\n",
    "\n",
    "So far this has worked splendidly... for the particular test cases we're looking at. This is an alpha-feature and we neither support all possible `pyiron_workflow` node types, nor have we searched for and tested possible failing edge-cases among the supported node types. Thus, there is a safety valve. To turn off ontological validation, just go to the root-most object and set `._validate_ontologies = False`:"
   ],
   "id": "15d876ba21cee422"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:57.506378Z",
     "start_time": "2025-09-12T15:17:57.500847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_silenced_ontology = pwf.Workflow(\"my_silenced_ontology\")\n",
    "my_silenced_ontology._validate_ontologies = False\n",
    "my_silenced_ontology.washed_clothes = wash(Clothes())\n",
    "my_silenced_ontology.dyed_clothes = dye_with_cancel(my_silenced_ontology.washed_clothes)\n",
    "my_silenced_ontology.money = sell(my_silenced_ontology.dyed_clothes)\n",
    "my_silenced_ontology()"
   ],
   "id": "93bfb07d4d377823",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'money__price': 10}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Node suggestions\n",
    "\n",
    "One of the advantages of graph-based workflows with hinted IO channels is facilitating guided workflow creation. Given a hinted channel instance in the context of some workflow, we can ask for suggestions of other channels with which to form a connection in the same, sibling graph context:"
   ],
   "id": "c612162f836d4c76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:57.593370Z",
     "start_time": "2025-09-12T15:17:57.521877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"ontoflow\")\n",
    "wf.make = prepare_pizza()\n",
    "wf.eat = eat_pizza()\n",
    "suggestions = suggest.suggest_connections(wf.eat.inputs.meal)\n",
    "for (node, channel) in suggestions:\n",
    "    print(node.full_label, channel.label)"
   ],
   "id": "7e01728625255664",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ontoflow/make pizza\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Similarly, given a corpus of node classes, we can ask for which nodes have at least one commensurate input/output with which our channel might connect. After adding such a node to our graph, we can leverage the connection suggester to see which channel(s) are appropriate.",
   "id": "c22ef9a507bb2230"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:57.673606Z",
     "start_time": "2025-09-12T15:17:57.600956Z"
    }
   },
   "cell_type": "code",
   "source": "suggest.suggest_nodes(wf.eat.inputs.meal, pwf.std.UserInput, prepare_pizza, wash)",
   "id": "4233ff3f954aa8ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[__main__.prepare_pizza]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Suggestion limitations\n",
    "\n",
    "When searching for new upstream nodes to add, the current implementation only looks at the immediate node, and not possible trees of upstream nodes. Returning to our clothes example, we can see that there is no _single_ suggestion for the `sell` node, because it requires clothes that are both dyed _and_ coloured, but our other nodes only provide one of these at a time!"
   ],
   "id": "afac4e0b565c801e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:57.685801Z",
     "start_time": "2025-09-12T15:17:57.678728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clothing_nodes = wash, dye, dye_with_cancel, sell\n",
    "\n",
    "wf = pwf.Workflow(\"working_backwards\")\n",
    "wf.money = sell()\n",
    "suggest.suggest_nodes(wf.money, *clothing_nodes)"
   ],
   "id": "df8ba4cf778b75d3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Of course working backwards a single step still works fine for lots of nodes, e.g. for `dye` we will take _anything_ that gives us clothes!",
   "id": "a32732a594de4540"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:57.956281Z",
     "start_time": "2025-09-12T15:17:57.693531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"single_step_back\")\n",
    "wf.dyed_clothes = dye()\n",
    "suggest.suggest_nodes(wf.dyed_clothes, *clothing_nodes)"
   ],
   "id": "eb0517eb5d999901",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[__main__.wash, __main__.dye, __main__.dye_with_cancel]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And when we look _downstream_ we have the advantage of knowing the entire upstream graph concretely, so there we are able to see options for fulfilling these more complex demands.",
   "id": "68c785add89b7f53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:58.545855Z",
     "start_time": "2025-09-12T15:17:57.961545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pwf.Workflow(\"downstream\")\n",
    "wf.dyed_clothes = dye(Clothes())\n",
    "wf.washed_clothes = wash(wf.dyed_clothes)\n",
    "suggestions = suggest.suggest_nodes(wf.washed_clothes, *clothing_nodes)\n",
    "assert(sell in suggestions)\n",
    "print(suggestions)"
   ],
   "id": "9e0064e26ce9e554",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class '__main__.wash'>, <class '__main__.dye'>, <class '__main__.dye_with_cancel'>, <class '__main__.sell'>]\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that inter-node connections do _not_ cause restrictions to propagate. Nicely, that means that if we have a node guarnateeing our immediate demands, we can get suggestions for it, even if it would itself introduce new demands. See below, where the middle node `GivesAndTakes` promises to fulfill the requirements of `TakesDownstream` even though it has its own needs:",
   "id": "38b026f5e536acdb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:17:58.648898Z",
     "start_time": "2025-09-12T15:17:58.551401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@pwf.as_function_node\n",
    "def GivesUpstreamNeed(x) -> u(str, uri=EX.Data, triples=(EX.has, EX.Upstream)):\n",
    "    return str(x)\n",
    "\n",
    "@pwf.as_function_node\n",
    "def GivesAndTakes(\n",
    "    y: u(\n",
    "        str,\n",
    "        uri=EX.Data,\n",
    "        restrictions=(\n",
    "            (rdflib.OWL.onProperty, EX.has),\n",
    "            (rdflib.OWL.someValuesFrom, EX.Upstream),\n",
    "        ),\n",
    "    ),\n",
    ") -> u(\n",
    "    str,\n",
    "    uri=EX.Data,\n",
    "    derived_from=\"inputs.y\",\n",
    "    triples=(EX.has, EX.Downstream)\n",
    "):\n",
    "    return y\n",
    "\n",
    "@pwf.as_function_node\n",
    "def TakesDownstream(\n",
    "    z: u(\n",
    "        str,\n",
    "        uri=EX.Data,\n",
    "        restrictions=(\n",
    "            (rdflib.OWL.onProperty, EX.has),\n",
    "            (rdflib.OWL.someValuesFrom, EX.Downstream),\n",
    "        ),\n",
    "    ),\n",
    ") -> str:\n",
    "    return z\n",
    "\n",
    "wf = pwf.Workflow(\"derived_restrictions\")\n",
    "wf.up = GivesUpstreamNeed()\n",
    "wf.middle = GivesAndTakes()\n",
    "wf.down = TakesDownstream()\n",
    "\n",
    "suggest.suggest_connections(wf.middle.outputs.y)"
   ],
   "id": "42fbb12d36c415d9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<__main__.TakesDownstream at 0x132987350>,\n",
       "  <pyiron_workflow.channels.InputData at 0x132986420>)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Complex workflows\n",
    "\n",
    "Ontological validation is still a new feature, and you may find an edge case we haven't found and tested yet. However, we can see below that even complex graphs including macros, dataclass nodes, and for-loops are able to validate and run:"
   ],
   "id": "9c3c3de6d21f80bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:18:02.119036Z",
     "start_time": "2025-09-12T15:17:58.666670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EX = rdflib.Namespace(\"http://www.example.org/\")\n",
    "\n",
    "@pwf.as_dataclass_node(uri=EX.Jar)  # Can pass u-kwargs to decorate the returned dataclass\n",
    "class Jar:\n",
    "    threading: str = \"clockwise\"\n",
    "    contents: u(str, uri=EX.Contents) = \"jam\"\n",
    "\n",
    "@pwf.as_function_node\n",
    "def ItsStuck(\n",
    "    jar: u(Jar.dataclass, uri=EX.Jar)\n",
    ") -> u(\n",
    "        Jar.dataclass,\n",
    "        uri=EX.Jar,\n",
    "        derived_from=\"inputs.jar\",\n",
    "        triples=(EX.lidState, EX.stuck)\n",
    "):\n",
    "    return jar\n",
    "\n",
    "@pwf.as_function_node\n",
    "def OpenStuckJar(\n",
    "    jar: u(\n",
    "        Jar.dataclass,\n",
    "        uri=EX.Jar,\n",
    "        restrictions=(\n",
    "            (rdflib.OWL.onProperty, EX.lidState),\n",
    "            (rdflib.OWL.someValuesFrom, EX.stuck),\n",
    "        ),\n",
    "    ),\n",
    ") -> u(str, uri=EX.Contents):\n",
    "    contents = jar.contents\n",
    "    return contents\n",
    "\n",
    "@pwf.as_function_node\n",
    "def MakeSandwich(made_with: u(str, uri=EX.Contents)) -> u(str, uri=EX.Sandwich):\n",
    "    sandwich = f\"{made_with} sandwich\"\n",
    "    return sandwich\n",
    "\n",
    "@pwf.as_macro_node\n",
    "def LunchTime(\n",
    "    self, contents: u(str, uri=EX.Contents)\n",
    ") -> u(str, uri=EX.Sandwich, triples=(EX.madeWith, \"inputs.contents\")):\n",
    "    self.jar = Jar(contents=contents)\n",
    "    self.stuck_jar = ItsStuck(self.jar)\n",
    "    self.open_jar = OpenStuckJar(self.stuck_jar)\n",
    "    self.lunch = MakeSandwich(self.open_jar)\n",
    "    return self.lunch\n",
    "\n",
    "\n",
    "\n",
    "wf = pwf.Workflow(\"lunch_for_three\")\n",
    "wf.platter = pwf.for_node(\n",
    "    body_node_class=LunchTime,\n",
    "    iter_on=\"contents\",\n",
    "    contents=[\"jam\", \"honey\", \"butter\"],\n",
    "    output_as_dataframe=False,\n",
    ")\n",
    "wf()"
   ],
   "id": "d3bfd2d5b856d187",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'platter__contents': ['jam', 'honey', 'butter'],\n",
       " 'platter__lunch': ['jam sandwich', 'honey sandwich', 'butter sandwich']}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Units\n",
    "\n",
    "`semantikon` annotations also allow us to specify physical units. When present, these are included in the ontological validation just like the other ontological terms.\n",
    "\n",
    "As such, we have no problem making same-unit connections:"
   ],
   "id": "7cf5e95f5ef33175"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:18:02.276165Z",
     "start_time": "2025-09-12T15:18:02.139675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@pwf.as_function_node\n",
    "def Distance(x: u(float, units=\"meter\")) -> u(float, derived_from=\"inputs.x\"):\n",
    "    return x\n",
    "\n",
    "@pwf.as_function_node\n",
    "def Speed(\n",
    "        dx: u(float, units=\"meter\"), dt: u(float, units=\"second\")\n",
    ") -> u(float, units=\"meter/second\"):\n",
    "    s = dx/dt\n",
    "    return s\n",
    "\n",
    "wf = pwf.Workflow(\"speedometer\")\n",
    "wf.dx = Distance(100)\n",
    "wf.speed = Speed(dx=wf.dx)"
   ],
   "id": "7fa30c5bb66141ee",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With incompatible units, we get an exception at connection time, just like with other ontological failures:",
   "id": "3c1f02dfc7a719a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:18:02.353940Z",
     "start_time": "2025-09-12T15:18:02.280458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@pwf.as_function_node\n",
    "def NanoTime(t: u(float, units=\"nanosecond\")) -> u(float, units=\"nanosecond\"):\n",
    "    return t\n",
    "\n",
    "wf.dt = NanoTime(10)\n",
    "try:\n",
    "    wf.speed.inputs.dt = wf.dt\n",
    "except ChannelConnectionError as e:\n",
    "    print(e)\n",
    "    wf.remove_child(wf.dt)"
   ],
   "id": "bad9905df584134c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The upstream channel /speedometer/dt.t cannot connect to the downstream channel /speedometer/speed.dt because the upstream type hint (typing.Annotated[float, ('units', 'nanosecond')]) and downstream type hint (typing.Annotated[float, ('units', 'second')]) produce a non-empty ontological validation report:\n",
      "{'missing_triples': [], 'incompatible_connections': [], 'distinct_units': {rdflib.term.URIRef('speedometer.dt.outputs.t.value'): [rdflib.term.URIRef('http://qudt.org/vocab/unit/NanoSEC'), rdflib.term.URIRef('http://qudt.org/vocab/unit/SEC')]}}\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With correct units, it works fine",
   "id": "1960f91b859f1516"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:18:02.507222Z",
     "start_time": "2025-09-12T15:18:02.365891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@pwf.as_function_node\n",
    "def Time(t: u(float, units=\"second\")) -> u(float, units=\"second\"):\n",
    "    return t\n",
    "\n",
    "wf.dt = Time(10)\n",
    "wf.speed.inputs.dt = wf.dt\n",
    "wf()"
   ],
   "id": "a918e2e7d80b8376",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speed__s': 10.0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that units are not inherited when `derived_from=` is used in the annotation; this is to easily allow for unit-converting nodes.",
   "id": "dafffcc1823b711f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Known Issues\n",
    "\n",
    "- This implementation naively creates a circular dependence: the `channels` module needs the `knowledge` module to evaluate the ontological validity of new connections, but the `knowledge` module relies on `workflow` and `nodes.composite` to parse graphs, and these in turn depend on `channels`. For now, we avoid dealing with this by importing `knowledge` locally in `channels` when it's time to use the ontology.\n",
    "- There are strings everywhere. The ontological features rely heavily on dictionaries, which are tough to type check and rely on string-based key access. E.g., when we want to see if the ontological validation raised any errors, we need to manually check on two dictionary entries by name. This is fragile.\n",
    "- It is inefficient. At every new ontologically-hinted connection, we reconstruct the entire recipe dictionary before positing the new connection and checking its validity. I'm not sure we'll get around validation operating on the entire graph, but we should adjust `pyiron_workflow` to store more recipe information at the class level where it is statically known (macros, function nodes, etc)\n",
    "- This is not fully edge-case tested. This works for the cases we show here and in the tests, but work in the underlying ontological validation in `semantikon` is ongoing, and there may be `pyiron_workflow`-specific syntax that is not yet playing well with `semantikon`. If you run into a problem, please raise a GitHub issue!\n",
    "- Upstream suggestions are limited to a _single_ suggestion, we don't do any tree construction to create upstream subgraphs that leverage trees of nodes in order to fulfill ontological demands. We _could_, but a brute-force attack would scale horribly with the node corpus.\n",
    "- Overhead: importing `semantikon.ontology` takes the better part of a second. We delay the import until the last moment, so this only impacts graphs where both ends of a connection are annotated, but there the import time is slow enough to be noticed on human scales.\n",
    "\n"
   ],
   "id": "c1e37915be9898fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T15:18:02.520567Z",
     "start_time": "2025-09-12T15:18:02.519351Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "67561c4f632db5a4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
